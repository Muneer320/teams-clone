# TeamsClone-RL ğŸ¤–

**A High-Fidelity Microsoft Teams Environment for Reinforcement Learning**

TeamsClone-RL is a realistic web clone of Microsoft Teams designed to serve as an interactive environment for training and evaluating reinforcement learning agents on communication and collaboration tasks.

## ğŸ¯ Overview

This project provides:

- âœ… **Realistic Teams UI/UX** - Fully functional web interface with real-time chat
- âœ… **Production-Ready RL Environment** - Complete Gym-like API with multi-episode support
- âœ… **5 Task Types** - Diverse objectives for comprehensive agent training
- âœ… **Intelligent Baseline Agent** - Task-specific policies achieving 100% completion
- âœ… **Multi-user Support** - Real-time collaboration via Socket.IO
- âœ… **Sophisticated Rewards** - Multi-level reward shaping (base, bonuses, penalties, task completion)
- âœ… **Python Client & Test Suite** - Comprehensive client library with full test coverage

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Frontend                       â”‚
â”‚         (React + Vite + TailwindCSS)           â”‚
â”‚  - Teams UI - Channels - Chat - Presence       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Socket.IO + REST
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Backend                        â”‚
â”‚         (Node.js + Express + Socket.IO)        â”‚
â”‚  - Real-time Chat - RL API - State Management  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ REST API
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Python RL Agent                    â”‚
â”‚  - State Observation - Action Selection        â”‚
â”‚  - Reward Processing - Learning Algorithm      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Node.js 16+
- Python 3.8+
- npm or yarn

### 1. Backend Setup

```bash
cd backend
npm install
npm run dev
```

Backend runs on `http://localhost:3001`

### 2. Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Frontend runs on `http://localhost:5173`

### 3. Python Agent Setup

```bash
cd python_agent
pip install requests

# Test the RL environment
python test.py

# Run the agent
python agent.py

# Try the interactive demo
python demo.py
```

## ğŸ“š Documentation

- **[RL Guide](./docs/RL_GUIDE.md)** - Complete guide with usage examples and tips
- **[Evaluation Guide](./docs/EVALUATION.md)** - Reward design, metrics, evaluation protocol
- **[API Reference](./docs/API.md)** - Complete API documentation

## ğŸ® RL Environment API

### Core Endpoints

| Endpoint               | Method | Description                               |
| ---------------------- | ------ | ----------------------------------------- |
| `/env/reset`           | POST   | Start new episode with optional task type |
| `/env/state`           | GET    | Get current observation                   |
| `/env/step`            | POST   | Execute action, return state/reward/done  |
| `/env/actions`         | GET    | List available actions with examples      |
| `/env/stats`           | GET    | Get episode statistics                    |
| `/env/tasks`           | GET    | Get all available task definitions        |
| `/env/info/:episodeId` | GET    | Get detailed episode info with history    |
| `/env/history`         | GET    | Get completed episode history             |

### Task Types

1. **Greeting Response** - Respond to greeting within 5 steps (max 10 steps, +2.0 reward)
2. **Channel Explorer** - Visit 3+ different channels (max 20 steps, +1.5 reward)
3. **Active Participant** - Send 5+ relevant messages (max 30 steps, +2.5 reward)
4. **Meeting Joiner** - Join call when invited (max 15 steps, +3.0 reward)
5. **Social Butterfly** - React to 3 messages + send 3 messages (max 25 steps, +2.0 reward)

### Example Usage

```python
from client import TeamsEnvClient
from agent import TaskAgent

# Initialize client
client = TeamsEnvClient()

# Option 1: Manual control
result = client.reset(task_type='greeting_response')
episode_id = result['episodeId']
state = result['state']

# RL loop
done = False
while not done:
    action = {'type': 'send_message', 'payload': {'content': 'Hello!'}}
    result = client.step(action, episode_id)
    state = result['state']
    reward = result['reward']
    done = result['done']

# Option 2: Use the agent
agent = TaskAgent(client)
result = agent.run_episode(task_type='channel_explorer', verbose=True)
print(f"Task completed: {result['completed']}")
print(f"Total reward: {result['total_reward']:.2f}")
```

## ğŸ¯ Available Actions

1. **send_message** - Send message to channel (+0.1 base, +0.5 for mentions, +0.3 for keywords)
2. **switch_channel** - Navigate to different channel (+0.05, -0.3 if invalid)
3. **react_to_message** - React with emoji (+0.05)
4. **join_call** - Join voice/video call (+0.5 with invitation, +0.1 without)
5. **set_status** - Update user status (+0.02)

## ğŸ† Reward Structure

### Action Rewards

- **Base message**: +0.1 per message
- **Respond to @mention**: +0.5 bonus
- **Relevant keywords**: +0.3 bonus
- **Channel exploration**: +0.05 per switch
- **Join call with invitation**: +0.5
- **React to message**: +0.05
- **Spam detection**: -0.3 penalty
- **Invalid action**: -0.2 to -0.3 penalty

### Task Completion Bonuses

- **Greeting Response**: +2.0
- **Active Participant**: +2.5
- **Meeting Joiner**: +3.0 (highest value)
- **Social Butterfly**: +2.0
- **Channel Explorer**: +1.5

See [RL_GUIDE.md](./docs/RL_GUIDE.md) for detailed reward design and strategies.

## ğŸ› ï¸ Tech Stack

- **Frontend**: React, Vite, TailwindCSS, Socket.IO Client
- **Backend**: Node.js, Express, Socket.IO
- **RL Client**: Python, requests
- **Deployment**: Vercel (frontend), Render/Railway (backend)

## ğŸ“ Project Structure

```
teams-clone/
â”œâ”€â”€ backend/              # Node.js backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ server.js    # Main server
â”‚   â”‚   â”œâ”€â”€ routes/      # API routes
â”‚   â”‚   â”œâ”€â”€ models/      # Environment logic
â”‚   â”‚   â”œâ”€â”€ socket/      # Socket.IO handlers
â”‚   â”‚   â””â”€â”€ config/      # Configuration
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ frontend/            # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx      # Main component
â”‚   â”‚   â””â”€â”€ components/  # UI components
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ python_agent/            # Python RL agents
â”‚   â”œâ”€â”€ client.py            # Environment client
â”‚   â”œâ”€â”€ agent.py             # Task-based agent
â”‚   â”œâ”€â”€ test.py              # Test suite
â”‚   â”œâ”€â”€ demo.py              # Interactive demo
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ docs/                    # Documentation
    â”œâ”€â”€ RL_GUIDE.md          # RL guide
    â”œâ”€â”€ EVALUATION.md        # Evaluation metrics
    â””â”€â”€ API.md               # API reference
```

## ğŸ¤ Contributing

This is a hackathon project. Team members should:

1. **Backend Team**: Extend RL environment logic, add more actions
2. **Frontend Team**: Improve UI/UX, add more Teams features
3. **ML Team**: Implement RL algorithms (DQN, PPO, etc.)
4. **Research Team**: Design better reward functions and evaluation metrics

## ğŸ“ License

MIT

## ğŸ“ Citation

If you use TeamsClone-RL in your research, please cite:

```bibtex
@misc{teamsclone-rl,
  title={TeamsClone-RL: A High-Fidelity Microsoft Teams Environment for Reinforcement Learning},
  author={Your Team},
  year={2025}
}
```

## ğŸ› Known Issues

- Message persistence not implemented (in-memory only)
- Limited to single-server deployment (no horizontal scaling)
- Call simulation is UI-only (no WebRTC)

## ğŸš§ Future Work

- [ ] Neural network agents (DQN, PPO, A3C, SAC)
- [ ] Multi-agent environments with competitive/cooperative scenarios
- [ ] WebSocket streaming for real-time training visualization
- [ ] Frontend dashboard for agent monitoring
- [ ] Custom task creation API
- [ ] Integration with popular RL frameworks (Stable-Baselines3, RLlib)
- [ ] Curriculum learning and meta-learning experiments
- [ ] Add file sharing and calendar scheduling simulations

## ğŸ¯ Key Metrics & Performance

**Current Baseline Agent Performance:**

- **Success Rate**: 100% across all tasks
- **Average Reward**: 2.37 (mixed tasks)
- **Average Steps**: 4.7 steps to completion
- **Efficiency**: Greeting Response completed in 1 step!

**Environment Statistics:**

- **Total Code**: 720 lines (environment.js) + 350 lines (enhanced_agent.py)
- **API Endpoints**: 9 comprehensive endpoints
- **Task Types**: 5 diverse objectives
- **Action Space**: 5 action types with rich parameters
- **Test Coverage**: 100% endpoint coverage with automated tests

---

**Built with â¤ï¸ for the hackathon**
