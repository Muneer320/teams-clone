{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a4b743",
   "metadata": {},
   "source": [
    "# TeamsClone-RL Training with Stable-Baselines3\n",
    "\n",
    "This notebook demonstrates how to train a reinforcement learning agent using PPO (Proximal Policy Optimization) on the TeamsClone environment.\n",
    "\n",
    "## Prerequisites\n",
    "- Backend server running on `http://localhost:3001`\n",
    "- Stable-Baselines3 and Gym installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a3017",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 gym matplotlib requests numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7426c",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Add python_agent to path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python_agent'))\n",
    "from client import TeamsEnvClient\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adead67",
   "metadata": {},
   "source": [
    "## 3. Create Gym-Compatible Environment Wrapper\n",
    "\n",
    "Wrap the TeamsEnvClient in a Gym environment for compatibility with Stable-Baselines3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d98609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamsGymEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for TeamsClone-RL\"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, base_url=\"http://localhost:3001\", task_type=\"greeting_response\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.client = TeamsEnvClient(base_url)\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # 5 discrete actions: send_message, switch_channel, react_to_message, join_call, set_status\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        \n",
    "        # Observation space: simplified state representation\n",
    "        # 10 features: [step_count, message_count, channel_id, user_status, ...]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=100, shape=(10,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.action_map = [\n",
    "            {\"type\": \"send_message\", \"content\": \"Hello!\"},\n",
    "            {\"type\": \"switch_channel\", \"channelId\": \"general\"},\n",
    "            {\"type\": \"react_to_message\", \"messageId\": \"msg-1\", \"reaction\": \"üëç\"},\n",
    "            {\"type\": \"join_call\"},\n",
    "            {\"type\": \"set_status\", \"status\": \"available\"}\n",
    "        ]\n",
    "        \n",
    "    def _get_obs(self, state_response):\n",
    "        \"\"\"Convert environment state to observation vector\"\"\"\n",
    "        state = state_response.get(\"state\", {})\n",
    "        \n",
    "        obs = np.zeros(10, dtype=np.float32)\n",
    "        obs[0] = state.get(\"stepCount\", 0)\n",
    "        obs[1] = len(state.get(\"messages\", []))\n",
    "        obs[2] = hash(state.get(\"currentChannel\", \"\")) % 100\n",
    "        obs[3] = hash(state.get(\"userPresence\", \"\")) % 10\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        episode_info = self.client.reset(task_type=self.task_type)\n",
    "        state_response = self.client.get_state()\n",
    "        return self._get_obs(state_response)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action in environment\"\"\"\n",
    "        # Map integer action to environment action\n",
    "        env_action = self.action_map[action]\n",
    "        \n",
    "        # Execute step\n",
    "        result = self.client.step(env_action)\n",
    "        \n",
    "        # Get new state\n",
    "        state_response = self.client.get_state()\n",
    "        obs = self._get_obs(state_response)\n",
    "        \n",
    "        reward = result.get(\"reward\", 0.0)\n",
    "        done = result.get(\"done\", False)\n",
    "        info = {\"step_result\": result}\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment (optional)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ TeamsGymEnv class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087e828",
   "metadata": {},
   "source": [
    "## 4. Create and Test Environment Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = TeamsGymEnv()\n",
    "\n",
    "# Test reset\n",
    "obs = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Initial observation: {obs}\")\n",
    "\n",
    "# Test single step\n",
    "obs, reward, done, info = env.step(0)\n",
    "print(f\"\\nAfter one step:\")\n",
    "print(f\"  Observation: {obs}\")\n",
    "print(f\"  Reward: {reward}\")\n",
    "print(f\"  Done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82da3a",
   "metadata": {},
   "source": [
    "## 5. Initialize PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac67a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in DummyVecEnv for Stable-Baselines3 compatibility\n",
    "vec_env = DummyVecEnv([lambda: TeamsGymEnv()])\n",
    "\n",
    "# Initialize PPO agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.99,\n",
    "    tensorboard_log=\"./ppo_teams_tensorboard/\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO agent initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25807fc5",
   "metadata": {},
   "source": [
    "## 6. Train the Agent\n",
    "\n",
    "Train for a small number of timesteps for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"üöÄ Starting training...\")\n",
    "model.learn(total_timesteps=2000)\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_teams_agent\")\n",
    "print(\"üíæ Model saved as 'ppo_teams_agent'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce661b01",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"ppo_teams_agent\")\n",
    "\n",
    "# Evaluate over multiple episodes\n",
    "num_eval_episodes = 5\n",
    "eval_rewards = []\n",
    "eval_steps = []\n",
    "\n",
    "for episode in range(num_eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done and step_count < 50:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    eval_rewards.append(total_reward)\n",
    "    eval_steps.append(step_count)\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward:.2f}, Steps = {step_count}\")\n",
    "\n",
    "print(f\"\\nüìä Average Reward: {np.mean(eval_rewards):.2f}\")\n",
    "print(f\"üìä Average Steps: {np.mean(eval_steps):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec42fb1",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66caf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(eval_rewards, marker='o', color='green')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('PPO Agent - Episode Rewards')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(eval_steps, marker='s', color='blue')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps')\n",
    "ax2.set_title('PPO Agent - Steps per Episode')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b801b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Creating a Gym-compatible wrapper for TeamsClone-RL\n",
    "2. Training a PPO agent using Stable-Baselines3\n",
    "3. Evaluating the trained agent's performance\n",
    "4. Visualizing training results\n",
    "\n",
    "### Next Steps\n",
    "- Tune hyperparameters for better performance\n",
    "- Train for more timesteps\n",
    "- Try different RL algorithms (DQN, A2C, SAC)\n",
    "- Improve state representation\n",
    "- Add curriculum learning for complex tasks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
